{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_entities(doc):\n",
    "    \"\"\"Preprocess a spaCy doc, merging entities into a single token.\n",
    "    Best used with nlp.add_pipe(merge_entities).\n",
    "\n",
    "    doc (spacy.tokens.Doc): The Doc object.\n",
    "    RETURNS (Doc): The Doc object with merged noun entities.\n",
    "    \"\"\"\n",
    "    spans = [(e.start_char, e.end_char, e.root.tag, e.root.dep, e.label)\n",
    "             for e in doc.ents]\n",
    "    for start, end, tag, dep, ent_type in spans:\n",
    "        doc.merge(start, end, tag=tag, dep=dep, ent_type=ent_type)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateparser as dp\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Support for maths\n",
    "import numpy as np\n",
    "# Plotting tools\n",
    "from matplotlib import pyplot as plt\n",
    "# we use the following for plotting figures in jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy_hunspell import spaCyHunSpell\n",
    "\n",
    "# load nlp parser with spellchecker\n",
    "nlp = spacy.load('en')\n",
    "hunspell = spaCyHunSpell(nlp, ('./src/hunspell/en_US.dic', './src/hunspell/en_US.aff'))\n",
    "\n",
    "nlp.add_pipe(hunspell)\n",
    "nlp.add_pipe(merge_entities, name='merge_entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tokenized_sentence = []\n",
    "    for token in sentence.split(' '): # simplest split is\n",
    "        token = ''.join(ch for ch in token if ch not in exclude)\n",
    "        if token != '':\n",
    "            tokenized_sentence.append(token.lower())\n",
    "    return ' '.join(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_spacy(parsed):\n",
    "    corrected = []\n",
    "    for w in parsed.doc:\n",
    "        if not(w._.hunspell_spell):\n",
    "            corrected.append(str(w._.hunspell_suggest[0]))\n",
    "        else:\n",
    "            corrected.append(str(w))\n",
    "    return nlp(' '.join(corrected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_sentence(sentence):\n",
    "    date_in_sentence = []\n",
    "    for entity in sentence.ents:\n",
    "        if entity.label_==\"DATE\":\n",
    "            date_in_sentence.append(entity.text)\n",
    "    return date_in_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date extraction\n",
    "def time_convert(list_str_date):\n",
    "    return [dp.parse(w) for w in list_str_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nsubj(sentence):\n",
    "    nsubj = []\n",
    "    for token in sentence:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.sentiment)\n",
    "        if token.dep_ == 'nsubj':\n",
    "            nsubj.append(token)\n",
    "    return nsubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"This is a test text, which mean i am avalable  the 24th of febraury, and tomorrow. But John will be here tuesday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test text which mean i am avalable the 24th of febraury and tomorrow but john will be here tuesday\n"
     ]
    }
   ],
   "source": [
    "## First Task, cleaning Data\n",
    "\n",
    "input_text = tokenize(input_text)\n",
    "\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, the text is parse with nlp from Spacy\n",
    "parsed = nlp(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test text which mean i am available the 24th of February and tomorrow but john will be here Tuesday\n"
     ]
    }
   ],
   "source": [
    "# Spellcheck\n",
    "corrected = correction_spacy(parsed)\n",
    "print(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the 24th of February', 'tomorrow', 'Tuesday']\n"
     ]
    }
   ],
   "source": [
    "# Date extraction\n",
    "dates_str = get_date_sentence(corrected)\n",
    "print(dates_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2019, 2, 24, 0, 0), datetime.datetime(2019, 1, 27, 22, 33, 56, 18574), datetime.datetime(2019, 1, 22, 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Date conversion\n",
    "dates = time_convert(dates_str)\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this this DET DT nsubj xxxx 0.0\n",
      "is be VERB VBZ ROOT xx 0.0\n",
      "a a DET DT det x 0.0\n",
      "test test NOUN NN compound xxxx 0.0\n",
      "text text NOUN NN attr xxxx 0.0\n",
      "which which ADJ WDT nsubj xxxx 0.0\n",
      "mean mean VERB VBP relcl xxxx 0.0\n",
      "i i PRON PRP nsubj x 0.0\n",
      "am be VERB VBP ccomp xx 0.0\n",
      "available available ADJ JJ acomp xxxx 0.0\n",
      "the 24th of February the NOUN NN npadvmod xxx ddxx xx Xxxxx 0.0\n",
      "and and CCONJ CC cc xxx 0.0\n",
      "tomorrow tomorrow NOUN NN conj xxxx 0.0\n",
      "but but CCONJ CC cc xxx 0.0\n",
      "john john PROPN NNP nsubj xxxx 0.0\n",
      "will will VERB MD aux xxxx 0.0\n",
      "be be VERB VB conj xx 0.0\n",
      "here here ADV RB advmod xxxx 0.0\n",
      "Tuesday tuesday PROPN NNP npadvmod Xxxxx 0.0\n",
      "[this, which, i, john]\n"
     ]
    }
   ],
   "source": [
    "# nsubj extraction \n",
    "nsubj = get_nsubj(corrected)\n",
    "print(nsubj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dep_graph(document):\n",
    "    edges = []\n",
    "    for token in document:\n",
    "        # FYI https://spacy.io/docs/api/token\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token),\n",
    "                          '{0}'.format(child)))\n",
    "\n",
    "    return nx.Graph(edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'the 24th of February'), ('i', 'tomorrow'), ('john', 'Tuesday')]\n"
     ]
    }
   ],
   "source": [
    "def get_subj_date(document, dates_str, subjects):\n",
    "    graph = get_dep_graph(document)\n",
    "    \n",
    "    paths =[[(subj.text, date) for subj in subjects ] for date in dates_str]\n",
    "    lengths =[[nx.shortest_path_length(graph, source=subj.text, target=date) for subj in nsubj ] for date in dates_str]\n",
    "    \n",
    "    index_min = [np.argmin(length) for length in lengths]\n",
    "    subject_date = []\n",
    "    for i, date in enumerate(dates_str):\n",
    "        subject_date.append(paths[i][index_min[i]])\n",
    "\n",
    "    return subject_date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
