{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateparser as dp\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_entities(doc):\n",
    "    \"\"\"Preprocess a spaCy doc, merging entities into a single token.\n",
    "    Best used with nlp.add_pipe(merge_entities).\n",
    "\n",
    "    doc (spacy.tokens.Doc): The Doc object.\n",
    "    RETURNS (Doc): The Doc object with merged noun entities.\n",
    "    \"\"\"\n",
    "    spans = [(e.start_char, e.end_char, e.root.tag, e.root.dep, e.label)\n",
    "             for e in doc.ents]\n",
    "    for start, end, tag, dep, ent_type in spans:\n",
    "        doc.merge(start, end, tag=tag, dep=dep, ent_type=ent_type)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_hunspell import spaCyHunSpell\n",
    "\n",
    "# load nlp parser with spellchecker\n",
    "nlp = spacy.load('en')\n",
    "hunspell = spaCyHunSpell(nlp, ('./src/hunspell/en_US.dic', './src/hunspell/en_US.aff'))\n",
    "\n",
    "nlp.add_pipe(hunspell)\n",
    "nlp.add_pipe(merge_entities, name='merge_entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    tokenized_sentence = []\n",
    "    for token in sentence.split(' '): # simplest split is\n",
    "        token = ''.join(ch for ch in token if ch not in exclude)\n",
    "        if token != '':\n",
    "            tokenized_sentence.append(token.lower())\n",
    "    return ' '.join(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction_spacy(parsed):\n",
    "    corrected = []\n",
    "    for w in parsed.doc:\n",
    "        if not(w.text in exclude) and not(w._.hunspell_spell):\n",
    "            if len(w._.hunspell_suggest)>0 :\n",
    "                corrected.append(w._.hunspell_suggest[0])\n",
    "            else:\n",
    "                corrected.append(w.text)\n",
    "        else:\n",
    "            corrected.append(str(w))\n",
    "    return nlp(' '.join(corrected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_sentence(sentence):\n",
    "    date_in_sentence = []\n",
    "    for entity in sentence.ents:\n",
    "        if entity.label_==\"DATE\":\n",
    "            date_in_sentence.append(entity.text)\n",
    "    return date_in_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date extraction\n",
    "def time_convert(list_str_date):\n",
    "    return [dp.parse(w) for w in list_str_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr(sentence, attribut):\n",
    "    nsubj = []\n",
    "    for token in sentence:\n",
    "        if token.dep_ == attribut:\n",
    "            nsubj.append(token)\n",
    "    return nsubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(sentence, pos):\n",
    "    nsubj = []\n",
    "    for token in sentence:\n",
    "        # print(token.text, token.pos_, token.dep_)\n",
    "        if token.pos_ == pos:\n",
    "            nsubj.append(token)\n",
    "    return nsubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dep_graph(document):\n",
    "    edges = []\n",
    "    for token in document:\n",
    "        # FYI https://spacy.io/docs/api/token\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token),\n",
    "                          '{0}'.format(child)))\n",
    "\n",
    "    return nx.Graph(edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_relation(graph, first_class, second_class):\n",
    "    paths =[[(elt2.text, elt1) for elt2 in second_class ] for elt1 in first_class]\n",
    "    lengths =[[nx.shortest_path_length(graph, source=subj.text, target=date) for subj in second_class ] for date in first_class]\n",
    "    \n",
    "    index_min = [np.argmin(length) for length in lengths]\n",
    "    subject_date = []\n",
    "    for i, date in enumerate(first_class):\n",
    "        subject_date.append(paths[i][index_min[i]])\n",
    "\n",
    "    return subject_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_is_negated(word):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    for child in word.children:\n",
    "        if child.dep_ == 'neg':\n",
    "            return True\n",
    "\n",
    "    if word.pos_ in {'VERB'}:\n",
    "        for ancestor in word.ancestors:\n",
    "            if ancestor.pos_ in {'VERB'}:\n",
    "                for child2 in ancestor.children:\n",
    "                    if child2.dep_ == 'neg':\n",
    "                        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def find_negated_wordSentIdxs_in_sent(sent, idxs_of_interest=None):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    negated_word_idxs = set()\n",
    "    for word_sent_idx, word in enumerate(sent):\n",
    "        if idxs_of_interest:\n",
    "            if word_sent_idx not in idxs_of_interest:\n",
    "                continue\n",
    "        if word_is_negated(word):\n",
    "            negated_word_idxs.add(word_sent_idx)\n",
    "\n",
    "    return negated_word_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(input_text):\n",
    "    ## First Task, simple cleaning for preparing proper tokenize\n",
    "    # input_text = tokenize(input_text)\n",
    "    \n",
    "    ## Next, the text is parse with nlp from Spacy\n",
    "    parsed = nlp(input_text)\n",
    "    corrected = correction_spacy(parsed)\n",
    "    \n",
    "    pos = []\n",
    "    neg = []\n",
    "    pos_str = []\n",
    "    neg_str = []\n",
    "    for sentence in corrected.sents:\n",
    "        print(\" \")\n",
    "        print(sentence.text)\n",
    "        # Spellcheck\n",
    "        parse_sent = nlp(sentence.text)\n",
    "        # Date extraction (as string)\n",
    "        dates_str = get_date_sentence(parse_sent)\n",
    "        # Extraction of subject and verbs (as token)\n",
    "        subjects = get_attr(parse_sent, \"nsubj\")\n",
    "        verbs = get_pos(parse_sent, \"VERB\")\n",
    "\n",
    "        # Get the graphs of dependancy (as nx graphs)\n",
    "        graph = get_dep_graph(parse_sent)\n",
    "        # Get the closest subjects and verbs\n",
    "        subj_date = get_closest_relation(graph, dates_str, subjects)\n",
    "        verb_date = get_closest_relation(graph, dates_str, verbs)\n",
    "        # Get all the negatives terms of the sentences\n",
    "        negatives = [parse_sent[negate].text for negate in find_negated_wordSentIdxs_in_sent(parse_sent)]\n",
    "\n",
    "        negative_time = [item[1] for item in verb_date if (item[0] in negatives or item[1] in negatives)]\n",
    "        positive_time = [item[1] for item in verb_date if not(item[1] in negative_time)]\n",
    "\n",
    "        pos_str += positive_time\n",
    "        neg_str += negative_time\n",
    "        neg += [ (key[0], dp.parse(key[1])) for key in subj_date if key[1] in negative_time]\n",
    "        pos += [ (key[0], dp.parse(key[1])) for key in subj_date if key[1] in positive_time]\n",
    "        \n",
    "    return pos, neg, pos_str, neg_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"This is a test text. which mean i am unavalable the 25th of februry, and also tomorrow. But John will not be here the 30th. Mary may be there frifay, but not wednesday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test text. which mean i am unavalable the 25th of februry, and also tomorrow. But John will not be here the 30th. Mary may be there frifay, but not wednesday\n",
      "\n",
      " \n",
      "This is a test text .\n",
      " \n",
      "which mean i am unavailable the 25th of February , and also tomorrow .\n",
      " \n",
      "But John will not be here the 30th .\n",
      " \n",
      "Mary may be there Friday , but not Wednesday\n",
      " \n",
      "Positive time and personne associated:\n",
      "the 25th of February\n",
      "('i', datetime.datetime(2019, 2, 25, 0, 0))\n",
      "tomorrow\n",
      "('i', datetime.datetime(2019, 1, 28, 3, 17, 24, 784813))\n",
      "Friday\n",
      "('Mary', datetime.datetime(2019, 1, 25, 0, 0))\n",
      " \n",
      "Negative time and personne associated:\n",
      "the 30th\n",
      "('John', datetime.datetime(2019, 1, 30, 0, 0))\n",
      "Wednesday\n",
      "('Mary', datetime.datetime(2019, 1, 23, 0, 0))\n"
     ]
    }
   ],
   "source": [
    "print(input_text)\n",
    "print(\"\")\n",
    "extraction_pos, extraction_neg, pos_str, neg_str = extract(input_text)\n",
    "print(\" \")\n",
    "print(\"Positive time and personne associated:\")\n",
    "for i, item in enumerate(extraction_pos):\n",
    "    print(pos_str[i])\n",
    "    print(item)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Negative time and personne associated:\")\n",
    "for i, item in enumerate(extraction_neg):\n",
    "    print(neg_str[i])\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
